{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_train.shape: torch.Size([7200, 6, 1, 64, 64])\n",
      "ds_val.shape: torch.Size([1800, 6, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# import torch  \n",
    "# import numpy as np\n",
    "# x_train = np.load(f'/data3/chengjingwen/diffusion-mdpnet/data/lo/uv.npy')\n",
    "# x_train = torch.tensor(x_train, dtype=torch.float32) \n",
    "\n",
    "# xmin = x_train.amin(dim=(0, 1, 3, 4), keepdim=True)  # 对轨迹数、时间步和空间维度求最小值\n",
    "# xmax = x_train.amax(dim=(0, 1, 3, 4), keepdim=True)  # 对轨迹数、时间步和空间维度求最大值\n",
    "\n",
    "\n",
    "# data = (x_train - xmin) / (xmax - xmin)\n",
    "# data = data[:,:,0:1,:,:]   # (num_tra, steps, 1, 64, 64)\n",
    "\n",
    "# # 滑动窗口划分 (groups, 6, 1, 64, 64)\n",
    "# pred_step = 5\n",
    "# num_trajectories, steps, num_var, x_dim, y_dim = data.shape\n",
    "# groups_per_trajectory = steps - pred_step  \n",
    "# groups = num_trajectories * groups_per_trajectory  \n",
    "\n",
    "\n",
    "# # 使用 sliding_window_view 进行窗口切分\n",
    "# data_groups = data.unfold(dimension=1, size=pred_step + 1, step=1)  # (num_trajectories, groups_per_trajectory, 6, 1, 64, 64)\n",
    "\n",
    "# data_groups = data_groups.permute(0,1,5,2,3,4)\n",
    "\n",
    "# data_groups = data_groups.reshape(-1, pred_step + 1, num_var, x_dim, y_dim)  # (groups, 6, 1, 64, 64)\n",
    "\n",
    "\n",
    "# num_train = int(0.8 * groups)  # 80% 训练集\n",
    "# num_val = groups - num_train\n",
    "\n",
    "# indices = torch.randperm(groups)  # 生成随机排列索引\n",
    "# train_indices = indices[:num_train]  \n",
    "# val_indices = indices[num_train:]\n",
    "# ds_train = data_groups[train_indices]\n",
    "# ds_val = data_groups[val_indices]\n",
    "\n",
    "# # 打印结果\n",
    "# print(f\"ds_train.shape: {ds_train.shape}\")  # 预期: (train_groups, 6, 1, 64, 64)\n",
    "# print(f\"ds_val.shape: {ds_val.shape}\")  # 预期: (val_groups, 6, 1, 64, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100, 2, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x_train = np.load(f'/data3/chengjingwen/diffusion-mdpnet/data/lo/uv.npy')\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32) \n",
    "print(x_train.shape)\n",
    "# 计算训练集的 min 和 max，维度为 (1, n_vars, 1, 1)\n",
    "xmin = x_train.amin(dim=(0, 1, 3, 4), keepdim=True)  # 对轨迹数、时间步和空间维度求最小值\n",
    "xmax = x_train.amax(dim=(0, 1, 3, 4), keepdim=True)  # 对轨迹数、时间步和空间维度求最大值\n",
    "\n",
    "\n",
    "data = (x_train - xmin) / (xmax - xmin)\n",
    "\n",
    "\n",
    "# print(f\"Training data normalized: min={data.min().item()}, max={data.max().item()}\")\n",
    "\n",
    "\n",
    "# torch.save({'xmin': xmin, 'xmax': xmax}, f'/data4/chengjingwen/kdd25-main 3/data/lo/normalization_params.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class MultiStepInputDataset(Dataset):\n",
    "    def __init__(self, data, history_steps=10):\n",
    "        \"\"\"\n",
    "        data: Tensor, shape (num_trajectories, num_steps, num_variables, x_dim, y_dim)\n",
    "        history_steps: how many past steps to use as input\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.history_steps = history_steps\n",
    "        self.num_trajectories, self.num_steps, self.num_variables, self.x_dim, self.y_dim = data.shape\n",
    "\n",
    "        # 确保有足够时间步能取 history_steps + 1（因为input要10步，output是第11步）\n",
    "        if self.num_steps <= self.history_steps:\n",
    "            raise ValueError(\"num_steps must be greater than history_steps\")\n",
    "\n",
    "    def __len__(self):\n",
    "        # 每条轨迹生成 (num_steps - history_steps) 个样本\n",
    "        return self.num_trajectories * (self.num_steps - self.history_steps)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 确定轨迹编号和在轨迹内的起点时间步\n",
    "        trajectory_idx = idx // (self.num_steps - self.history_steps)\n",
    "        time_idx = idx % (self.num_steps - self.history_steps)\n",
    "\n",
    "        # input: 连续 history_steps 个时间步，堆叠成channel\n",
    "        input_data = self.data[trajectory_idx, time_idx : time_idx + self.history_steps]  # (history_steps, num_variables, x_dim, y_dim)\n",
    "        \n",
    "\n",
    "        # target: 预测的第 history_steps 后的一个时间步\n",
    "        target_data = self.data[trajectory_idx, time_idx + self.history_steps].unsqueeze(0)  # (1, num_variables, x_dim, y_dim)\n",
    "\n",
    "        return input_data, target_data\n",
    "\n",
    "\n",
    "# train_data = data\n",
    "\n",
    "# train_dataset = MultiStepInputDataset(train_data, history_steps=10)\n",
    "\n",
    "# # 随机划分 80% train, 20% val\n",
    "# train_size = int(0.8 * len(train_dataset))\n",
    "# val_size = len(train_dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# # 打印一个batch看看\n",
    "# for inputs, targets in train_loader:\n",
    "#     print(f\"Input shape: {inputs.shape}, Target shape: {targets.shape}\")\n",
    "#     num_variables = inputs.shape[1]\n",
    "#     break  # 只打印一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.global_avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y  # Apply the attention weights\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=(kernel_size - 1) // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        y = torch.cat([avg_out, max_out], dim=1)  # Concatenate along channel dimension\n",
    "        y = self.conv(y)\n",
    "        return x * self.sigmoid(y)\n",
    "\n",
    "class ChannelSpatialSELayer(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels, reduction)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.channel_attention(x)\n",
    "        x = self.spatial_attention(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, groups=in_channels)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"A basic residual block with optional downsampling.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1, Separable=False, CBAM=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        conv = DepthwiseSeparableConv if Separable else nn.Conv2d\n",
    "        self.cbam = ChannelSpatialSELayer(out_channels) if CBAM else nn.Identity()\n",
    "        \n",
    "        self.conv1 = conv(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut for matching dimensions\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride) \\\n",
    "            if stride != 1 or in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x = (x + shortcut) / math.sqrt(2.)  # Residual connection\n",
    "        x = self.cbam(x)  # Ensure CBAM has correct input channels\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels, latent_dim, nx, ny, Separable=False, CBAM=False):\n",
    "        super().__init__()\n",
    "        self.nx, self.ny = nx, ny\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            ResidualBlock(input_channels, 64, stride=2, Separable=Separable, CBAM=CBAM),\n",
    "            ResidualBlock(64, 128, stride=2, Separable=Separable, CBAM=CBAM),\n",
    "            ResidualBlock(128, 256, stride=2, Separable=Separable, CBAM=CBAM),\n",
    "            ResidualBlock(256, 512, stride=2, Separable=Separable, CBAM=CBAM),\n",
    "        )\n",
    "\n",
    "        self.flatten_size = (nx // 16) * (ny // 16) * 512\n",
    "        self.fc = nn.Linear(self.flatten_size, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_channels, latent_dim, nx, ny, CBAM=False):\n",
    "        super().__init__()\n",
    "        self.nx, self.ny = nx, ny\n",
    "\n",
    "        self.fc = nn.Linear(latent_dim, 512 * (nx // 16) * (ny // 16))\n",
    "        self.unflatten = nn.Unflatten(1, (512, nx // 16, ny // 16))\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            ChannelSpatialSELayer(256) if CBAM else\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            ChannelSpatialSELayer(128) if CBAM else\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            ChannelSpatialSELayer(64) if CBAM else\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, output_channels, kernel_size=4, stride=2, padding=1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = self.unflatten(x)\n",
    "        return self.features(x)\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"Complete Autoencoder model.\"\"\"\n",
    "    def __init__(self, input_channels, latent_dim, nx, ny, Separable=False, CBAM=False):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_channels, latent_dim, nx, ny, Separable=Separable, CBAM=CBAM)\n",
    "        self.decoder = Decoder(input_channels, latent_dim, nx, ny)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n",
    "# # Test the model\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Input dimensions\n",
    "#     input_channels = 2  # For example, RGB=3, grayscale=1, etc.\n",
    "#     latent_dim = 256\n",
    "#     nx, ny = 64, 64  # Height and Width of input images\n",
    "\n",
    "#     # Create autoencoder\n",
    "#     autoencoder = Autoencoder(input_channels, latent_dim, nx, ny, Separable=True, CBAM=True).to(\"cuda:1\")\n",
    "\n",
    "#     # Test input\n",
    "#     x = torch.randn(16, input_channels, nx, ny).to(\"cuda:1\")  # Batch of 16 images\n",
    "#     reconstructed = autoencoder(x)\n",
    "\n",
    "#     # Ensure output shape matches input shape\n",
    "#     print(f\"Input shape: {x.shape}\")\n",
    "#     print(f\"Reconstructed shape: {reconstructed.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, n=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n = n  # Default prediction steps\n",
    "        self.cell = nn.LSTM(\n",
    "            input_size=input_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            dropout=0, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "        self.activation = nn.ReLU()  # Use ReLU activation\n",
    "\n",
    "    def forward(self, x, n=None):\n",
    "        if n is None:  # Use default n if not provided\n",
    "            n = self.n\n",
    "        \n",
    "        device = x.device\n",
    "        batch_size = x.size(0)  # Get batch size\n",
    "        seq_length = x.size(1) if len(x.shape) > 2 else 1  # Get sequence length\n",
    "        \n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, dtype=torch.float32, device=device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Add time dimension if input is 2D\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)  # [batch_size, 1, input_dim]\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        outputs, (h, c) = self.cell(x, (h0, c0))\n",
    "        \n",
    "        # Apply ReLU activation to LSTM outputs\n",
    "        outputs = self.activation(outputs)  # [batch_size, seq_length, hidden_dim]\n",
    "\n",
    "        # Decode the last hidden state output\n",
    "        y = [self.fc(outputs[:, -1, :])]  # [batch_size, input_dim]\n",
    "        \n",
    "        # Multi-step prediction\n",
    "        for _ in range(1, n):\n",
    "            last_output = y[-1].unsqueeze(1)  # [batch_size, 1, input_dim]\n",
    "            outputs, (h, c) = self.cell(last_output, (h, c))\n",
    "            outputs = self.activation(outputs)  # Apply ReLU activation\n",
    "            y.append(self.fc(outputs[:, -1, :]))\n",
    "        \n",
    "        # Return last prediction and all predictions\n",
    "        return y[-1], torch.stack(y, dim=1)  # [batch_size, n, input_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 10, 2, 128, 128]), Target shape: torch.Size([32, 1, 2, 128, 128])\n",
      "模型总参数量: 37.10 M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "input_step = 10\n",
    "pred_step = 1\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "latent_dim = 512\n",
    "device = \"cuda:7\" \n",
    "lr_min = 1e-5\n",
    "\n",
    "train_data = data\n",
    "\n",
    "train_dataset = MultiStepInputDataset(train_data, history_steps=10)\n",
    "\n",
    "# 随机划分 80% train, 20% val\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 打印一个batch看看\n",
    "for inputs, targets in train_loader:\n",
    "    print(f\"Input shape: {inputs.shape}, Target shape: {targets.shape}\")\n",
    "    num_variables = inputs.shape[2]\n",
    "    break  # 只打印一次\n",
    "\n",
    "autoencoder = Autoencoder(input_channels=num_variables, latent_dim=latent_dim, nx=128, ny=128, Separable=True, CBAM=True).to(device)\n",
    "lstm = LSTM(input_dim=latent_dim, hidden_dim=latent_dim, num_layers=2, n=pred_step).to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in autoencoder.parameters())\n",
    "print(f\"模型总参数量: {num_params / 1e6:.2f} M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Reconstruction Loss: 1.19e-04\n",
      "learning rate for lstm:0.0001\n",
      "Epoch [2/10], Reconstruction Loss: 1.14e-04\n",
      "learning rate for lstm:0.0001\n",
      "Epoch [3/10], Reconstruction Loss: 1.11e-04\n",
      "learning rate for lstm:0.0001\n",
      "Epoch [4/10], Reconstruction Loss: 1.10e-04\n",
      "learning rate for lstm:0.0001\n",
      "Epoch [5/10], Reconstruction Loss: 1.09e-04\n",
      "learning rate for lstm:0.0001\n",
      "Epoch [6/10], Reconstruction Loss: 1.04e-04\n",
      "learning rate for lstm:0.0001\n",
      "Epoch [7/10], Reconstruction Loss: 1.03e-04\n",
      "learning rate for lstm:0.0001\n",
      "Epoch [8/10], Reconstruction Loss: 1.03e-04\n",
      "learning rate for lstm:0.0001\n",
      "Epoch [9/10], Reconstruction Loss: 9.97e-05\n",
      "learning rate for lstm:0.0001\n",
      "Epoch [10/10], Reconstruction Loss: 9.57e-05\n",
      "learning rate for lstm:0.0001\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = Adam(list(autoencoder.parameters()) + list(lstm.parameters()), lr=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()\n",
    "    lstm.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (input_data, target_data) in enumerate(train_loader):\n",
    "       \n",
    "        input_data = input_data.to(device)  # Shape: [batch_size, input_step, num_variables, x_dim, y_dim]\n",
    "        target_data = target_data.to(device)  # Shape: [batch_size, pred_step, num_variables, x_dim, y_dim]\n",
    "\n",
    "       \n",
    "        batch_size, input_step, num_variables, x_dim, y_dim = input_data.shape\n",
    "        input_data_flat = input_data.view(-1, num_variables, x_dim, y_dim)  # Shape: [batch_size * input_step, num_variables, x_dim, y_dim]\n",
    "        target_data_flat = target_data.view(-1, num_variables, x_dim, y_dim)\n",
    "\n",
    "       \n",
    "        latent_features = autoencoder.encoder(input_data_flat)  # Shape: [batch_size * input_step, latent_dim]\n",
    "        latent_features = latent_features.view(batch_size, input_step, -1)  # Shape: [batch_size, input_step, latent_dim]\n",
    "        _, lstm_outputs = lstm(latent_features, n=pred_step)  # Shape: [batch_size, pred_step, latent_dim]\n",
    "\n",
    "        lstm_output_flat = lstm_outputs.view(-1, lstm_outputs.size(-1))  # Shape: [batch_size * pred_step, latent_dim]\n",
    "        reconstructed = autoencoder.decoder(lstm_output_flat)  # Shape: [batch_size * pred_step, num_variables, x_dim, y_dim]\n",
    "        \n",
    "        \n",
    "        loss = criterion(reconstructed, target_data_flat)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    scheduler.step(avg_loss)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Reconstruction Loss: {avg_loss:.2e}\")\n",
    "    print(f\"learning rate for lstm:{optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "    if optimizer.param_groups[0]['lr'] < lr_min:\n",
    "        print(f\"Learning rate below threshold ({lr_min}). Stopping early.\")\n",
    "        break    \n",
    "       \n",
    "\n",
    "    \n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# torch.save(autoencoder.state_dict(), f\"/data3/chengjingwen/diffusion-mdpnet/trained_model/baseline/lo_autoencoder_latent_{latent_dim}.pth\")\n",
    "# torch.save(lstm.state_dict(), f\"/data3/chengjingwen/diffusion-mdpnet/trained_model/baseline/lo_lstm_latent_{latent_dim}.pth\")\n",
    "# print(\"Models saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data normalized: min=0.0, max=1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x_test = np.load(f'/data3/chengjingwen/diffusion-mdpnet/data/lo/uv.npy')\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32) \n",
    "\n",
    "\n",
    "\n",
    "x_train = np.load(f'/data3/chengjingwen/diffusion-mdpnet/data/lo/uv.npy')\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32) \n",
    "\n",
    "# 计算训练集的 min 和 max，维度为 (1, n_vars, 1, 1)\n",
    "xmin = x_train.amin(dim=(0, 1, 3, 4), keepdim=True)  # 对轨迹数、时间步和空间维度求最小值\n",
    "xmax = x_train.amax(dim=(0, 1, 3, 4), keepdim=True)  # 对轨迹数、时间步和空间维度求最大值\n",
    "\n",
    "del x_train\n",
    "x_test_normalized = (x_test - xmin) / (xmax - xmin)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Test data normalized: min={x_test_normalized.min().item()}, max={x_test_normalized.max().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 132\u001b[39m\n\u001b[32m    129\u001b[39m test_data = x_test_normalized.to(device).float()[:\u001b[32m1\u001b[39m]  \n\u001b[32m    130\u001b[39m \u001b[38;5;28mprint\u001b[39m(test_data.shape)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m results = test_model(autoencoder, lstm, test_data, device)\n\u001b[32m    135\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNMSE Mean: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[33m'\u001b[39m\u001b[33mNMSE_mean\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNMSE Std: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[33m'\u001b[39m\u001b[33mNMSE_std\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mtest_model\u001b[39m\u001b[34m(autoencoder, lstm, test_data, device)\u001b[39m\n\u001b[32m     89\u001b[39m         plt.imshow(torch.abs(targets[t, \u001b[32m0\u001b[39m] - predictions[t, \u001b[32m0\u001b[39m]).detach().cpu().numpy(), cmap=\u001b[33m\"\u001b[39m\u001b[33mjet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     90\u001b[39m         plt.colorbar()\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m         plt.show()\n\u001b[32m     93\u001b[39m         clear_output(wait=\u001b[38;5;28;01mTrue\u001b[39;00m)  \n\u001b[32m     97\u001b[39m ssim_values = ssim(predictions, targets, data_range=\u001b[32m1.0\u001b[39m).item() \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/matplotlib/pyplot.py:614\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    570\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    571\u001b[39m \u001b[33;03mDisplay all open figures.\u001b[39;00m\n\u001b[32m    572\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    611\u001b[39m \u001b[33;03mexplicitly there.\u001b[39;00m\n\u001b[32m    612\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    613\u001b[39m _warn_if_gui_out_of_main_thread()\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _get_backend_mod().show(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/matplotlib_inline/backend_inline.py:90\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(close, block)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf.get_all_fig_managers():\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m         display(\n\u001b[32m     91\u001b[39m             figure_manager.canvas.figure,\n\u001b[32m     92\u001b[39m             metadata=_fetch_figure_metadata(figure_manager.canvas.figure)\n\u001b[32m     93\u001b[39m         )\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     95\u001b[39m     show._to_draw = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/IPython/core/display_functions.py:278\u001b[39m, in \u001b[36mdisplay\u001b[39m\u001b[34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[39m\n\u001b[32m    276\u001b[39m     publish_display_data(data=obj, metadata=metadata, **kwargs)\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     format_dict, md_dict = \u001b[38;5;28mformat\u001b[39m(obj, include=include, exclude=exclude)\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[32m    280\u001b[39m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[32m    281\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/IPython/core/formatters.py:238\u001b[39m, in \u001b[36mDisplayFormatter.format\u001b[39m\u001b[34m(self, obj, include, exclude)\u001b[39m\n\u001b[32m    236\u001b[39m md = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     data = formatter(obj)\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/decorator.py:232\u001b[39m, in \u001b[36mdecorate.<locals>.fun\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[32m    231\u001b[39m     args, kw = fix(args, kw, sig)\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, *(extras + args), **kw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/IPython/core/formatters.py:282\u001b[39m, in \u001b[36mcatch_format_error\u001b[39m\u001b[34m(method, self, *args, **kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     r = method(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[32m    284\u001b[39m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/IPython/core/formatters.py:402\u001b[39m, in \u001b[36mBaseFormatter.__call__\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m printer(obj)\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[32m    404\u001b[39m method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170\u001b[39m, in \u001b[36mprint_figure\u001b[39m\u001b[34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[39m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[32m    168\u001b[39m     FigureCanvasBase(fig)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m fig.canvas.print_figure(bytes_io, **kw)\n\u001b[32m    171\u001b[39m data = bytes_io.getvalue()\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fmt == \u001b[33m'\u001b[39m\u001b[33msvg\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/matplotlib/backend_bases.py:2158\u001b[39m, in \u001b[36mFigureCanvasBase.print_figure\u001b[39m\u001b[34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[39m\n\u001b[32m   2156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[32m   2157\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches == \u001b[33m\"\u001b[39m\u001b[33mtight\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2158\u001b[39m         bbox_inches = \u001b[38;5;28mself\u001b[39m.figure.get_tightbbox(\n\u001b[32m   2159\u001b[39m             renderer, bbox_extra_artists=bbox_extra_artists)\n\u001b[32m   2160\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(layout_engine, ConstrainedLayoutEngine) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m   2161\u001b[39m                 pad_inches == \u001b[33m\"\u001b[39m\u001b[33mlayout\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2162\u001b[39m             h_pad = layout_engine.get()[\u001b[33m\"\u001b[39m\u001b[33mh_pad\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/matplotlib/figure.py:1848\u001b[39m, in \u001b[36mFigureBase.get_tightbbox\u001b[39m\u001b[34m(self, renderer, bbox_extra_artists)\u001b[39m\n\u001b[32m   1844\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ax.get_visible():\n\u001b[32m   1845\u001b[39m     \u001b[38;5;66;03m# some Axes don't take the bbox_extra_artists kwarg so we\u001b[39;00m\n\u001b[32m   1846\u001b[39m     \u001b[38;5;66;03m# need this conditional....\u001b[39;00m\n\u001b[32m   1847\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1848\u001b[39m         bbox = ax.get_tightbbox(\n\u001b[32m   1849\u001b[39m             renderer, bbox_extra_artists=bbox_extra_artists)\n\u001b[32m   1850\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1851\u001b[39m         bbox = ax.get_tightbbox(renderer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/matplotlib/axes/_base.py:4548\u001b[39m, in \u001b[36m_AxesBase.get_tightbbox\u001b[39m\u001b[34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[39m\n\u001b[32m   4546\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._axis_map.values():\n\u001b[32m   4547\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.axison \u001b[38;5;129;01mand\u001b[39;00m axis.get_visible():\n\u001b[32m-> \u001b[39m\u001b[32m4548\u001b[39m         ba = martist._get_tightbbox_for_layout_only(axis, renderer)\n\u001b[32m   4549\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m ba:\n\u001b[32m   4550\u001b[39m             bb.append(ba)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/matplotlib/artist.py:1402\u001b[39m, in \u001b[36m_get_tightbbox_for_layout_only\u001b[39m\u001b[34m(obj, *args, **kwargs)\u001b[39m\n\u001b[32m   1396\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1397\u001b[39m \u001b[33;03mMatplotlib's `.Axes.get_tightbbox` and `.Axis.get_tightbbox` support a\u001b[39;00m\n\u001b[32m   1398\u001b[39m \u001b[33;03m*for_layout_only* kwarg; this helper tries to use the kwarg but skips it\u001b[39;00m\n\u001b[32m   1399\u001b[39m \u001b[33;03mwhen encountering third-party subclasses that do not support it.\u001b[39;00m\n\u001b[32m   1400\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.get_tightbbox(*args, **{**kwargs, \u001b[33m\"\u001b[39m\u001b[33mfor_layout_only\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n\u001b[32m   1403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1404\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.get_tightbbox(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/matplotlib/axis.py:1353\u001b[39m, in \u001b[36mAxis.get_tightbbox\u001b[39m\u001b[34m(self, renderer, for_layout_only)\u001b[39m\n\u001b[32m   1350\u001b[39m     renderer = \u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m)._get_renderer()\n\u001b[32m   1351\u001b[39m ticks_to_draw = \u001b[38;5;28mself\u001b[39m._update_ticks()\n\u001b[32m-> \u001b[39m\u001b[32m1353\u001b[39m \u001b[38;5;28mself\u001b[39m._update_label_position(renderer)\n\u001b[32m   1355\u001b[39m \u001b[38;5;66;03m# go back to just this axis's tick labels\u001b[39;00m\n\u001b[32m   1356\u001b[39m tlb1, tlb2 = \u001b[38;5;28mself\u001b[39m._get_ticklabel_bboxes(ticks_to_draw, renderer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/matplotlib/axis.py:2686\u001b[39m, in \u001b[36mYAxis._update_label_position\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m   2682\u001b[39m     \u001b[38;5;28mself\u001b[39m.label.set_position(\n\u001b[32m   2683\u001b[39m         (bbox.x0 - \u001b[38;5;28mself\u001b[39m.labelpad * \u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m).dpi / \u001b[32m72\u001b[39m, y))\n\u001b[32m   2684\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2685\u001b[39m     \u001b[38;5;66;03m# Union with extents of the right spine if present, of the axes otherwise.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2686\u001b[39m     bbox = mtransforms.Bbox.union([\n\u001b[32m   2687\u001b[39m         *bboxes2, \u001b[38;5;28mself\u001b[39m.axes.spines.get(\u001b[33m\"\u001b[39m\u001b[33mright\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.axes).get_window_extent()])\n\u001b[32m   2688\u001b[39m     \u001b[38;5;28mself\u001b[39m.label.set_position(\n\u001b[32m   2689\u001b[39m         (bbox.x1 + \u001b[38;5;28mself\u001b[39m.labelpad * \u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m).dpi / \u001b[32m72\u001b[39m, y))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/matplotlib/transforms.py:649\u001b[39m, in \u001b[36mBboxBase.union\u001b[39m\u001b[34m(bboxes)\u001b[39m\n\u001b[32m    647\u001b[39m x1 = np.max([bbox.xmax \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m bboxes])\n\u001b[32m    648\u001b[39m y0 = np.min([bbox.ymin \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m bboxes])\n\u001b[32m--> \u001b[39m\u001b[32m649\u001b[39m y1 = np.max([bbox.ymax \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m bboxes])\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Bbox([[x0, y0], [x1, y1]])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/matplotlib/transforms.py:317\u001b[39m, in \u001b[36mBboxBase.ymax\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mymax\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    316\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"The top edge of the bounding box.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.max(\u001b[38;5;28mself\u001b[39m.get_points()[:, \u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3164\u001b[39m, in \u001b[36mmax\u001b[39m\u001b[34m(a, axis, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   3052\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[32m   3053\u001b[39m \u001b[38;5;129m@set_module\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   3054\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmax\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue, initial=np._NoValue,\n\u001b[32m   3055\u001b[39m          where=np._NoValue):\n\u001b[32m   3056\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3057\u001b[39m \u001b[33;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[32m   3058\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3162\u001b[39m \u001b[33;03m    5\u001b[39;00m\n\u001b[32m   3163\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np.maximum, \u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out,\n\u001b[32m   3165\u001b[39m                           keepdims=keepdims, initial=initial, where=where)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/lmenv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86\u001b[39m, in \u001b[36m_wrapreduction\u001b[39m\u001b[34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis=axis, out=out, **passkwargs)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#### AE的自回归为什么这么差？？？ check\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchmetrics.functional import structural_similarity_index_measure as ssim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "pred_step = 1  \n",
    "pred_steps = 20\n",
    "latent_dim = 512\n",
    "start_time = 10\n",
    "\n",
    "\n",
    "def compute_nmse(predictions, targets):\n",
    "    \"\"\"计算 NMSE\"\"\"\n",
    "    return torch.mean(((predictions - targets) ** 2)) / torch.mean((targets ** 2))\n",
    "\n",
    "def compute_rmse(predictions, targets):\n",
    "    \"\"\"计算 RMSE\"\"\"\n",
    "    mse = torch.mean((predictions - targets) ** 2)\n",
    "    return torch.sqrt(mse)\n",
    "\n",
    "\n",
    "def test_model(autoencoder, lstm, test_data, device):\n",
    "    autoencoder.eval()\n",
    "    lstm.eval()\n",
    "\n",
    "    num_trajectories, T, num_variables, x_dim, y_dim = test_data.shape\n",
    "\n",
    "    nmse_list = []\n",
    "    ssim_list = []\n",
    "    rmse_list = []\n",
    "\n",
    "    \n",
    "    for trajectory_idx in range(num_trajectories):\n",
    "        trajectory = test_data[trajectory_idx]  # shape [100, num_variables, x_dim, y_dim]\n",
    "        # start_time = np.random.randint(0, 40)  # 随机选起始时间\n",
    "        \n",
    "        targets = trajectory[start_time :start_time + pred_steps].to(device)  # 未来 50 步\n",
    "\n",
    "        # 初始化预测结果\n",
    "        predictions = torch.zeros((pred_steps, num_variables, x_dim, y_dim)).to(device)\n",
    "        predictions_encoded = torch.zeros((pred_steps, latent_dim)).to(device)\n",
    "\n",
    "        for t in range(0,pred_steps,pred_step):  \n",
    "            if t == 0:\n",
    "                # 起始输入通过 Autoencoder 压缩\n",
    "                input_data = trajectory[start_time - 10: start_time].to(device)  # shape: [10, num_variables, x_dim, y_dim]\n",
    "                latent = autoencoder.encoder(input_data).unsqueeze(0)  # shape: [1, 10, latent_dim]\n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "            \n",
    "            # LSTM 多步预测\n",
    "            # print(latent[0,0])\n",
    "            _, latent_preds = lstm(latent, n=pred_step)  # shape: [1, 1, latent_dim]\n",
    "            latent_preds = latent_preds.squeeze(0)  # shape: [pred_step, latent_dim]\n",
    "            latent = latent[0,1:]  # shape: [10, latent_dim]\n",
    "            # print(latent.shape, latent_preds.shape)\n",
    "            latent = torch.cat([latent, latent_preds], dim = 0).unsqueeze(0)\n",
    "            \n",
    "            predictions_encoded[t:t+pred_step] = latent_preds\n",
    "            \n",
    "            \n",
    "            predictions[t:t+pred_step] = autoencoder.decoder(latent_preds)\n",
    "            # ipdb.set_trace()\n",
    "            \n",
    "        if trajectory_idx == 0:\n",
    "            for t in range(90):\n",
    "                plt.figure(figsize=(12, 4))\n",
    "\n",
    "                # Ground Truth\n",
    "                plt.subplot(131)\n",
    "                plt.title(f\"Ground Truth (step={ t + 1})\")\n",
    "                plt.imshow(targets[t, 0].detach().cpu().numpy(), cmap=\"coolwarm\", vmin=0, vmax=1) \n",
    "                plt.colorbar()\n",
    "\n",
    "                # Prediction\n",
    "                plt.subplot(132)\n",
    "                plt.title(\"Prediction\")\n",
    "                plt.imshow(predictions[t, 0].detach().cpu().numpy(), cmap=\"coolwarm\", vmin=0, vmax=1)  \n",
    "                plt.colorbar()\n",
    "\n",
    "                # Difference and MSE\n",
    "                mse = torch.mean((targets[t, 0] - predictions[t, 0]) ** 2).item() \n",
    "                plt.subplot(133)\n",
    "                plt.title(f\"Difference (MSE={mse:.5f})\")\n",
    "                plt.imshow(torch.abs(targets[t, 0] - predictions[t, 0]).detach().cpu().numpy(), cmap=\"jet\")\n",
    "                plt.colorbar()\n",
    "\n",
    "                plt.show()\n",
    "                clear_output(wait=True)  \n",
    "            \n",
    "\n",
    "       \n",
    "        ssim_values = ssim(predictions, targets, data_range=1.0).item() \n",
    "        nmse_trajectory = compute_nmse(predictions, targets).item() \n",
    "        rmse_trajectory = compute_rmse(predictions, targets).item() \n",
    "\n",
    "        nmse_list.append(nmse_trajectory)\n",
    "        ssim_list.append(ssim_values)\n",
    "        rmse_list.append(rmse_trajectory)\n",
    "\n",
    "    nmse_mean, nmse_std = np.mean(nmse_list), np.std(nmse_list)\n",
    "    ssim_mean, ssim_std = np.mean(ssim_list), np.std(ssim_list)\n",
    "    rmse_mean, rmse_std = np.mean(rmse_list), np.std(rmse_list)\n",
    "\n",
    "    return {\n",
    "        \"NMSE_mean\": nmse_mean,\n",
    "        \"NMSE_std\": nmse_std,\n",
    "        \"SSIM_mean\": ssim_mean,\n",
    "        \"SSIM_std\": ssim_std,\n",
    "        \"RMSE_mean\": rmse_mean,\n",
    "        \"RMSE_std\": rmse_std\n",
    "    }\n",
    "\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "autoencoder = Autoencoder(input_channels=2, latent_dim=latent_dim, nx=128, ny=128, Separable=True, CBAM=True).to(device)\n",
    "lstm = LSTM(input_dim=latent_dim, hidden_dim=latent_dim, num_layers=2, n=1).to(device)\n",
    "\n",
    "autoencoder.load_state_dict(torch.load(f\"/data3/chengjingwen/diffusion-mdpnet/trained_model/baseline/lo_autoencoder_latent_512.pth\"))\n",
    "lstm.load_state_dict(torch.load(f\"/data3/chengjingwen/diffusion-mdpnet/trained_model/baseline/lo_lstm_latent_512.pth\"))\n",
    "\n",
    "\n",
    "test_data = x_test_normalized.to(device).float()[:1]  \n",
    "print(test_data.shape)\n",
    "\n",
    "results = test_model(autoencoder, lstm, test_data, device)\n",
    "\n",
    "\n",
    "print(f\"NMSE Mean: {results['NMSE_mean']}\")\n",
    "print(f\"NMSE Std: {results['NMSE_std']}\")\n",
    "print(f\"SSIM Mean: {results['SSIM_mean']}\")\n",
    "print(f\"SSIM Std: {results['SSIM_std']}\")\n",
    "print(f\"RMSE Mean: {results['RMSE_mean']}\")\n",
    "print(f\"RMSE Std: {results['RMSE_std']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型总参数量: 10.89 M\n"
     ]
    }
   ],
   "source": [
    "model = lstm.to(\"cuda:3\")\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"模型总参数量: {num_params / 1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 50, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
